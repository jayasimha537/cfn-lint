"""
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
SPDX-License-Identifier: MIT-0
"""
import fnmatch
import json
import logging
import multiprocessing
import os
import re
import subprocess
import warnings
import zipfile
from io import BytesIO
from typing import Dict
from urllib.request import Request, urlopen

import jsonpatch

import cfnlint
import cfnlint.data.AdditionalSpecs
from cfnlint.helpers import (
    SPEC_REGIONS,
    apply_json_patch,
    get_url_content,
    url_has_newer_version,
)
from cfnlint.schema.manager import PROVIDER_SCHEMA_MANAGER

LOGGER = logging.getLogger(__name__)

REGISTRY_SCHEMA_ZIP = (
    "https://schema.cloudformation.us-east-1.amazonaws.com/CloudformationSchema.zip"
)


def update_resource_specs(force: bool = False):
    # Pool() uses cpu count if no number of processors is specified
    # Pool() only implements the Context Manager protocol from Python3.3 onwards,
    # so it will fail Python2.7 style linting, as well as throw AttributeError

    # Update provider Schemas
    PROVIDER_SCHEMA_MANAGER.update(force)


def update_documentation(rules):
    # Update the overview of all rules in the linter
    filename = "docs/rules.md"

    # Sort rules by the Rule ID
    sorted_rules = sorted(rules, key=lambda obj: obj.id)

    data = []

    # Read current file up to the Rules part, everything up to that point is
    # static documentation.
    with open(filename, "r", encoding="utf-8") as original_file:

        line = original_file.readline()
        while line:
            data.append(line)

            if line == "## Rules\n":
                break

            line = original_file.readline()

    # Rebuild the file content
    with open(filename, "w", encoding="utf-8") as new_file:

        # Rewrite the static documentation
        for line in data:
            new_file.write(line)

        # Add the rules
        new_file.write(
            "(_This documentation is generated by running `cfn-lint --update-documentation`, do not alter this manually_)\n\n"
        )
        new_file.write(
            f"The following **{len(sorted_rules) + 3}** rules are applied by this linter:\n\n"
        )
        new_file.write(
            "| Rule ID  | Title | Description | Config<br />(Name:Type:Default) | Source | Tags |\n"
        )
        new_file.write(
            "| -------- | ----- | ----------- | ---------- | ------ | ---- |\n"
        )

        rule_output = (
            '| [{0}<a name="{0}"></a>]({6}) | {1} | {2} | {3} | [Source]({4}) | {5} |\n'
        )

        for rule in [
            cfnlint.rules.ParseError(),
            cfnlint.rules.TransformError(),
            cfnlint.rules.RuleError(),
        ] + sorted_rules:
            rule_source_code_file = (
                "../"
                + subprocess.check_output(
                    [
                        "git",
                        "grep",
                        "-l",
                        'id = "' + rule.id + '"',
                        "src/cfnlint/rules/",
                    ]
                )
                .decode("ascii")
                .strip()
            )
            rule_id = rule.id + "*" if rule.experimental else rule.id
            tags = ",".join(f"`{tag}`" for tag in rule.tags)
            config = "<br />".join(
                f'{key}:{values.get("type")}:{values.get("default")}'
                for key, values in rule.config_definition.items()
            )
            new_file.write(
                rule_output.format(
                    rule_id,
                    rule.shortdesc,
                    rule.description,
                    config,
                    rule.source_url,
                    tags,
                    rule_source_code_file,
                )
            )
        new_file.write("\n\\* experimental rules\n")


def patch_spec(content: Dict, region: str):
    """Patch the spec file"""
    LOGGER.info('Patching spec file for region "%s"', region)

    append_dir = os.path.join(
        os.path.dirname(__file__), "data", "ExtendedSpecs", region
    )
    for dirpath, _, filenames in os.walk(append_dir):
        filenames.sort()
        for filename in fnmatch.filter(filenames, "*.json"):
            file_path = os.path.basename(filename)
            module = dirpath.replace(f"{append_dir}", f"{region}").replace(
                os.path.sep, "."
            )
            LOGGER.info("Processing %s/%s", module, file_path)
            all_patches = jsonpatch.JsonPatch(
                cfnlint.helpers.load_resource(
                    f"cfnlint.data.ExtendedSpecs.{module}", file_path
                )
            )

            content = apply_json_patch(content, all_patches, region)

    return content


def update_iam_policies():
    """update iam policies file"""

    url = "https://awspolicygen.s3.amazonaws.com/js/policies.js"

    filename = os.path.join(
        os.path.dirname(cfnlint.data.AdditionalSpecs.__file__), "Policies.json"
    )
    LOGGER.debug("Downloading policies %s into %s", url, filename)

    content = get_url_content(url)

    content = content.split("app.PolicyEditorConfig=")[1]
    content = json.loads(content)

    actions = {
        "Manage Amazon API Gateway": ["HEAD", "OPTIONS"],
        "Amazon API Gateway Management": ["HEAD", "OPTIONS"],
        "Amazon API Gateway Management V2": ["HEAD", "OPTIONS"],
        "Amazon Kinesis Video Streams": ["StartStreamEncryption"],
    }
    for k, v in actions.items():
        if content.get("serviceMap").get(k):
            content["serviceMap"][k]["Actions"].extend(v)
        else:
            LOGGER.debug('"%s" was not found in the policies file', k)

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(content, f, indent=1, sort_keys=True, separators=(",", ": "))


def get_schema_value_types():
    def resolve_refs(properties, schema):
        results = {}
        name = None

        if properties.get("$ref"):
            name = properties.get("$ref").split("/")[-1]
            subname, results = resolve_refs(schema.get("definitions").get(name), schema)
            if subname:
                name = subname
            properties = schema.get("definitions").get(name)

        if properties.get("type") == "array":
            results = properties.get("items")

        if results:
            properties = results

        if results and results.get("$ref"):
            name, results = resolve_refs(results, schema)

        if not results:
            return name, properties

        return name, results

    def get_object_details(names, properties, schema):
        results = {}
        warnings.filterwarnings("error")
        for propname, propdetails in properties.items():
            subname, propdetails = resolve_refs(propdetails, schema)
            t = propdetails.get("type")
            if not t:
                continue
            if t in ["object"]:
                if subname is None:
                    subname = propname
                if propdetails.get("properties"):
                    if subname not in names:
                        results.update(
                            get_object_details(
                                names + [subname], propdetails.get("properties"), schema
                            )
                        )
                elif (
                    propdetails.get("oneOf")
                    or propdetails.get("anyOf")
                    or propdetails.get("allOf")
                ):
                    LOGGER.info(
                        "Type %s object for %s has only oneOf,anyOf, or allOf properties",
                        names[0],
                        propname,
                    )
                    continue
            elif t not in ["string", "integer", "number", "boolean"]:
                if propdetails.get("$ref"):
                    results.update(
                        get_object_details(
                            names + [propname],
                            schema.get("definitions").get(t.get("$ref").split("/")[-1]),
                            schema,
                        )
                    )
                elif isinstance(t, list):
                    LOGGER.info(
                        "Type for %s object and %s property is a list",
                        names[0],
                        propname,
                    )
                else:
                    LOGGER.info(
                        "Unable to handle %s object for %s property", names[0], propname
                    )
            elif t == "string":
                if not results.get(".".join(names + [propname])):
                    if (
                        propdetails.get("pattern")
                        or (
                            propdetails.get("minLength")
                            and propdetails.get("maxLength")
                        )
                        or propdetails.get("enum")
                    ):
                        results[".".join(names + [propname])] = {}
                if propdetails.get("pattern"):
                    p = propdetails.get("pattern")
                    if (
                        ".".join(names + [propname])
                        == "AWS::OpsWorksCM::Server.CustomPrivateKey"
                    ):
                        # one off exception to handle a weird parsing issue in python 2.7
                        continue
                    # python 3 has the ability to test isascii
                    # python 3.7 introduces is ascii so switching to encode
                    try:
                        p.encode("ascii")
                    except UnicodeEncodeError:
                        continue
                    try:
                        if "\\p{" in p:
                            continue
                        re.compile(p, re.UNICODE)
                        results[".".join(names + [propname])].update(
                            {"AllowedPatternRegex": p}
                        )
                    except:  # pylint: disable=bare-except
                        LOGGER.info(
                            "Unable to handle regex for type %s and property %s with regex %s",
                            names[0],
                            propname,
                            p,
                        )
                if propdetails.get("minLength") and propdetails.get("maxLength"):
                    results[".".join(names + [propname])].update(
                        {
                            "StringMin": propdetails.get("minLength"),
                            "StringMax": propdetails.get("maxLength"),
                        }
                    )
                if propdetails.get("enum"):
                    results[".".join(names + [propname])].update(
                        {"AllowedValues": propdetails.get("enum")}
                    )
            elif t in ["number", "integer"]:
                if not results.get(".".join(names + [propname])):
                    if propdetails.get("minimum") and propdetails.get("maximum"):
                        results[".".join(names + [propname])] = {}
                if propdetails.get("minimum") and propdetails.get("maximum"):
                    results[".".join(names + [propname])].update(
                        {
                            "NumberMin": propdetails.get("minimum"),
                            "NumberMax": propdetails.get("maximum"),
                        }
                    )

        return results

    def process_schema(schema):
        details = get_object_details(
            [schema.get("typeName")], schema.get("properties"), schema
        )

        # Remove duplicates
        vtypes = {}
        for n, v in details.items():
            if n.count(".") > 1:
                s = n.split(".")
                vtypes[s[0] + "." + ".".join(s[-2:])] = v
            else:
                vtypes[n] = v

        patches = []
        for n, v in vtypes.items():
            patch = []
            if v:
                if n.count(".") == 2:
                    r_type = "PropertyTypes"
                else:
                    r_type = "ResourceTypes"
                element = {
                    "op": "add",
                    "path": f'/{r_type}/{".".join(n.split(".")[0:-1])}/Properties/{n.split(".")[-1]}/Value',
                    "value": {
                        "ValueType": n,
                    },
                }
                patch.append(element)
                for s, vs in v.items():
                    element = {
                        "op": "add",
                        "path": f"/ValueTypes/{n}/{s}",
                        "value": vs,
                    }
                    patch.append(element)
            if patch:
                patches.append(patch)

        return patches

    req = Request(REGISTRY_SCHEMA_ZIP)

    results = []
    with urlopen(req) as res:
        with zipfile.ZipFile(BytesIO(res.read())) as z:
            for f in z.namelist():
                with z.open(f) as d:
                    if not isinstance(d, str):
                        data = d.read()
                    else:
                        data = d
                    if isinstance(data, bytes):
                        data = data.decode("utf-8")
                    schema = json.loads(data)
                    patches = process_schema(schema)
                    results.extend(patches)

    return results
